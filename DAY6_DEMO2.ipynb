{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install langchain langchain-openai langchain-community langchain-text-splitters chromadb sentence-transformers -q\n",
        "print(\"LangChain + Embeddings Installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvd01wHDyAJp",
        "outputId": "3c554729-0ea7-4962-8b64-87e4649af93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLangChain + Embeddings Installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im49o4EDs2-y"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os, numpy as np\n",
        "NEBIUS_KEY = userdata.get(\"NEBIUS_API_KEY\") or os.getenv(\"NEBIUS_API_KEY\")\n",
        "if not NEBIUS_KEY:\n",
        "    raise RuntimeError(\"Put NEBIUS_API_KEY into Colab Runtime Variables\")\n",
        "NEBIUS_BASE = \"https://api.studio.nebius.com/v1\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI( model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", base_url=\"https://api.studio.nebius.com/v1\", api_key=NEBIUS_KEY, temperature=0.0, )\n",
        "print(\"Nebius Chat Model Ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmd5Z3r_zYRm",
        "outputId": "b12e6951-a348-4b03-efba-446efe859a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nebius Chat Model Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rtWVtQRx_ql",
        "outputId": "c2de3750-f994-4720-ab3f-27abf3414f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "\n",
        "# --- Sample corpus ---\n",
        "texts = [\n",
        "    \"Library rules: penalty fee for late books is $10 per week.\",\n",
        "    \"Attendance rule: students must attend 75% of classes to be eligible for exams.\",\n",
        "    \"How to apply for class leave: submit a form to the office.\",\n",
        "    \"Library timings: Mon-Fri 9AM to 7PM, Sat 10AM to 4PM.\",\n",
        "    \"Academic calendar: semester dates, holidays, and exam schedules.\"\n",
        "]\n",
        "\n",
        "# --- Create BM25 retriever from raw texts ---\n",
        "bm25 = BM25Retriever.from_texts(texts)\n",
        "bm25.k = 3  # number of top docs to retrieve\n",
        "\n",
        "# --- Query ---\n",
        "query = \"library penalty fee rules\"\n",
        "\n",
        "results = bm25.get_relevant_documents(query)  # returns small dict objects internally\n",
        "\n",
        "print(f\"\\nğŸ” Query: {query}\")\n",
        "print(f\"Top {bm25.k} Results (BM25):\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"[{i}] {r.page_content}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GhNHCvgv_Vu",
        "outputId": "2394bf5d-249d-4ba7-b3ae-d91cf2feca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Query: library penalty fee rules\n",
            "Top 3 Results (BM25):\n",
            "\n",
            "[1] Library rules: penalty fee for late books is $10 per week.\n",
            "[2] Academic calendar: semester dates, holidays, and exam schedules.\n",
            "[3] Library timings: Mon-Fri 9AM to 7PM, Sat 10AM to 4PM.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-796434691.py:20: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = bm25.get_relevant_documents(query)  # returns small dict objects internally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers.ensemble import EnsembleRetriever\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# --- Your corpus ---\n",
        "docs_text = [\n",
        "    \"Students must maintain 75% attendance in each course, except CIE. attendance doesnt matter for the CIE course\",\n",
        "    \"students must File an FIR at the nearest policestation on losing thier ID cards\",\n",
        "    \"Jio works from 02 to 05\",\n",
        "    \"Medical exemptions must be submitted within 7 days of return.\"\n",
        "]\n",
        "\n",
        "# --- (1) Dense retriever using your code ---\n",
        "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectordb = Chroma.from_texts(docs_text, embedding=emb, collection_name=\"college_kb\")\n",
        "dense_retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "print(\"âœ… Vector DB Created with\", len(docs_text), \"documents\")\n",
        "\n",
        "# --- (2) BM25 retriever (lexical) ---\n",
        "bm25_retriever = BM25Retriever.from_texts(docs_text)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "# --- (3) Hybrid retriever ---\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, dense_retriever],\n",
        "    weights=[0.4, 0.6]\n",
        "    k=3\n",
        ")\n",
        "\n",
        "# --- (4) Query ---\n",
        "query = \"attendance exemption rules for CIE\"\n",
        "results = hybrid_retriever.get_relevant_documents(query)\n",
        "\n",
        "print(f\"\\nğŸ” Query: {query}\")\n",
        "print(f\"Top {len(results)} results (Hybrid BM25 + Dense):\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"[{i}] {r.page_content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjqkPMLkzDr-",
        "outputId": "35b38ede-e2ae-4734-c5a8-2425cda86795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vector DB Created with 4 documents\n",
            "\n",
            "ğŸ” Query: attendance exemption rules for CIE\n",
            "Top 2 results (Hybrid BM25 + Dense):\n",
            "\n",
            "[1] Students must maintain 75% attendance in each course, except CIE. attendance doesnt matter for the CIE course\n",
            "[2] Medical exemptions must be submitted within 7 days of return.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rag_query_nebius.py\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        \"You are a helpful assistant for college policy questions.\\n\\n\"\n",
        "        \"Context:\\n{context}\\n\\n\"\n",
        "        \"Question:\\n{question}\\n\\n\"\n",
        "        \"Answer clearly using only the context above.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# --- build the RAG chain ---\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=hybrid_retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "# --- query ---\n",
        "query = \"what happens if I miss 75 percent attendance in CIE?\"\n",
        "answer = qa_chain.run(query)\n",
        "\n",
        "print(\"\\nğŸ” Query:\", query)\n",
        "print(\"\\nğŸ§  Answer:\\n\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkzUmOamzZX-",
        "outputId": "b2a6d5e4-ad5d-4d87-8c78-902dbf9fd814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2930952775.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa_chain.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Query: what happens if I miss 75 percent attendance in CIE?\n",
            "\n",
            "ğŸ§  Answer:\n",
            " Since attendance doesn't matter for the CIE course, missing 75% attendance in CIE does not have any consequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmEAOXAk0Gy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TASK\n"
      ],
      "metadata": {
        "id": "wHhZnUN30Hl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into pairs (2 per laptop). Run the provided cells.\n",
        "\n",
        "For each query in queries (and add one of your own), record:\n",
        "\n",
        "Top-2 BM25 results (copy exact text)\n",
        "\n",
        "Top-2 Dense results\n",
        "\n",
        "Top-3 Hybrid results\n",
        "\n",
        "For each query, write 1 sentence: which retriever gave the best answer and why (keyword match vs semantic).\n",
        "\n",
        "Use the Nebius LLM qa.run(...) for one query to produce a final answer; check if the LLM's answer uses correct context.\n",
        "\n",
        "Submit: a short text file or a single Google Doc with the three queries, the retriever outputs, and your 3 short conclusions."
      ],
      "metadata": {
        "id": "T6NOEui10MxE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRAD8ydo0LWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hX6S_2t30K93"
      }
    }
  ]
}